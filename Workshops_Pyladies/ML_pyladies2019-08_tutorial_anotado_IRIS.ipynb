#!/usr/bin/env python
# coding: utf-8

# # ML_pyladies 2019-08 - São Paulo
# # tutorial_anotado_iris_COPIA 
# # based on  Liliane Nakazono workshop
# 
#

'''
1. Bibliotecas
'''
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix
import itertools
from sklearn import metrics


# In[2]:


'''
2. Funcoes
'''

# Definindo funções:
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.3f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()


# In[ ]:





# In[9]:


'''
3. Leitura de dados

Iris dataset
'''
# load_iris([return_X_y])

# input data
iris = pd.DataFrame(datasets.load_iris().data, columns=datasets.load_iris().feature_names) 

# output expected (real data)
target =  pd.DataFrame(datasets.load_iris().target, columns=['target'])


# In[ ]:





# In[16]:


datasets.load_iris().data


# In[15]:


datasets.load_iris().feature_names


# In[18]:


# Target:
pd.DataFrame(datasets.load_iris().target, columns=['target'])


# In[ ]:


'''================================================================

4. Pré-análise dos dados

================================================================='''


# In[37]:


type(iris)


# In[19]:


# lista nome das colunas:
list(iris)


# In[21]:


# muda nome das colunas:
iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']


# In[ ]:





# In[22]:


#checa tipo de cada coluna:
iris.dtypes


# In[23]:


# checa se tem NaNs
iris.isna().sum()


# In[25]:


# retira linhas com NaNs
iris = iris.dropna()

# para retirar  n/a...
# muito cuidado para nao jogar fora dados que poderia ser importantes  !!!!!!!!
# checar antes se nao seria melhor imputar o 


# In[33]:


# estatisticas:
iris.describe()


# In[35]:


iris.describe().T


# In[38]:


iris.sepal_length.mean() #média


# In[39]:


# acessando linhas/colunas:
iris.iloc[ 0:5, :]


# In[42]:


iris.head(4)


# In[41]:


iris.iloc[:,0]


# In[48]:


# iris.hist(bins=20)
# plt.show()

data = pd.concat([iris,target], axis=1)
# sns.pairplot(data, hue='target')


###### 
#data[0:10,0]  ERRADO
data[0:10]


# In[51]:


iris[0:5]


# In[60]:


#filtrando a tabela segundo determinadas condições
iris_2 = iris.query('petal_length>1 & petal_width>1 & sepal_length>1 & sepal_width>1')
# iris_2 = iris[(iris.petal_length>1) & (iris.petal_width>1) & (iris.sepal_length>1) & (iris.sepal_width>1)]

### extra
print(len(iris_2))
iris_2[0:10]


# In[63]:


# acessa por label:   ---> LOC
iris_2.loc[50:52,:]  #---> Localiza identificador 50:52


# In[57]:


### extra
# .loc é diferente do index... loc acessa por label.
iris_2[50:52]   # localiza linhas com índicelab 50:52...


# In[68]:


# acessa por posição    -> ILOC
iris_2.iloc[0:3,]  # = iris_2.iloc[0:3,:]


# In[70]:


# nova tabela com apenas algumas colunas:
petal = iris[['petal_length', 'petal_width']]
petal


# In[72]:


#matriz de correlação:
'''
Pandas dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe. 
Any na values are automatically excluded. 
For any non-numeric data type columns in the dataframe it is ignored.
Note: The correlation of a variable with itself is 1.
'''
iris.corr()


# In[95]:


#plt.figure(figsize=(8, 8))
sns.heatmap (iris.corr(), cmap='bwr', vmax=1, vmin=-1, center=0, square=True, annot=True)

'''
***8about graph cut error
#This was a matplotlib regression introduced in 3.1.1 
which has been fixed in 3.1.2 (still forthcoming). 
For now the fix is to downgrade matplotlib to a prior version.

***about categorical data overlap in axis
Here we have the classic problem with categorical data:
we need to display all the labels and because some of them are quite long, they overlap.
''' 


# In[97]:


plt.ylim() 


# In[104]:


###EXTRA SOLUTION
# fix for mpl bug that cuts off top/bottom of seaborn viz

#sns.heatmap (iris.corr(), cmap='bwr', vmax=1, vmin=-1, center=0, square=True, annot=True)
chart=sns.heatmap (iris.corr(), cmap='bwr', vmax=1, vmin=-1, center=0, square=True, annot=True)
chart.set_xticklabels(chart.get_xticklabels(), rotation=45)

b, t = plt.ylim() # discover the values for bottom and top
b += 0.5 # Add 0.5 to the bottom
t -= 0.5 # Subtract 0.5 from the top
plt.ylim(b, t) # update the ylim(bottom, top) values
plt.show() # ta-da!


# In[105]:


#Imprimir tabela:

print(iris)


# In[106]:


#Nome dos atributos (colunas):

list(iris)


# In[94]:


#Contagem de linhas para um certo atributo:

#iris.atribute.value_counts()
    
iris.sepal_length.value_counts()


# In[89]:


#Tipo dos atributos:

iris.dtypes


# In[92]:


#Estatísticas:

iris.describe().T
#valores muito altos de desvio padrao, pode ser algum fator muito discrepante , pode ter outliers, 


# In[108]:


# Média de um atributo:

#iris.atributo.mean()
iris.petal_length.mean()


# In[109]:


#Desvio padrão de um atributo:

iris.petal_length.std()


# In[112]:


iris.describe().T[0,1]
# NAO FUNCIONA


# In[136]:


#Concatenar linhas/colunas:     
iris2=iris             
iris2.concat()  ############ NAO FUNCIONOU


# In[119]:


data = pd.concat([iris,target], axis=1)
data


# In[120]:


#Checar se tem NaNs:

iris.isna().sum()


# In[121]:


#Remover NaNs:

iris.dropna()


# In[125]:


#Acessar todas as colunas da linha em posição i:

#iris.iloc[i,:]
iris.iloc[10,:]  # ID 10, linha 11


# In[128]:


iris.loc[11,:]   # ID11 ( linha 12)


# In[129]:


#Acessar todas as colunas da linha de índice i:
# iris.loc[i,:]

iris.loc[1,:]    #  1	4.9	3.0	1.4	0.2


# In[130]:


#Filtrando tabela:
# Parte 1

#iris.query('condição1 & condição2')

# 'petal_length>1 & petal_width>1 & sepal_length>1 & sepal_width>1'

iris.query('petal_length>2 & sepal_length>2')


# In[139]:


#Filtrando tabela:
# Parte 2
#iris[(condição1) & (condição2)]

#  iris[(iris.petal_length>2) & (iris.sepal_length>2)]############ NAO FUNCIONOU
iris[(iris.petal_length>2) & (iris.sepal_length>2)]  #########assim funcionou!!!


# In[140]:


#Histograma:

iris.hist()


# In[146]:


list(data)


# In[147]:


list(iris)


# In[149]:


target


# In[148]:


# Pairplot:
sns.pairplot(data, hue="target")   ###NAO FUNCIONOU
# data contém uma coluna chamada target com as classes


# sns.pairplot(data, hue='target') ## TB NAO FUNCIONOU


# In[153]:


sns.pairplot(data)


# In[154]:


# Matriz de correlação:

iris.corr()


# In[155]:


#Visualização matriz de correlação:

sns.heatmap(iris.corr(), cmap='bwr', vmax=1, vmin=-1,center=0, square=True, annot=True)


# In[ ]:





# In[ ]:





# In[ ]:


'''================================================================

5.Amostragem para validação cruzada

================================================================='''


# In[163]:


# Criando amostra de treinamento (70%) e de teste (30%):
X_train, X_test, y_train, y_test = train_test_split(iris, 
                                                    target, 
                                                    test_size = 0.3,
                                                    random_state = 2)
# X contem o iris
# Y contem o target


# In[173]:


X_train


# In[177]:


#novamente vamos rodar
X_train, X_test, y_train, y_test = train_test_split(iris, 
                                                    target, 
                                                    test_size = 0.3,
                                                    random_state = 2)


# In[178]:


X_train

# o resultado é igual ao anteior , pq usamos a mesma "regra de randomização": random_state_2


# In[161]:


# Para fazer uma amostragem estratificada, adicionar o parâmetro:

stratify = target
#target: tem informaçòes das classes

#amostragem aleatorio: no micro tudo é "pseudo-aleatorio".
#random state é  um numero que sempre fornece as mesmas amostras de teste e treinamento.
#     a mesma regra de random_state=2 dáos mesmos resultados

###vamos checar : 


# In[185]:


'''================================================================

6. Validação de modelo

================================================================='''

'''
========Random Forest
''' 
clf = RandomForestClassifier()

clf.fit(X_train, y_train.values.ravel())


# In[203]:


y_pred1 = clf.predict(X_test)   #Valores preditos pelo modelo

# matrix = confusion_matrix(y_test, y_pred)
# fig = plot_confusion_matrix(matrix, 
#                     classes=['0', '1', '2'])
# plt.show()

print(metrics.classification_report
      (y_test,y_pred1))


# In[181]:


y_train.values.ravel()


# In[204]:


matrix1= confusion_matrix(y_test,y_pred1)

fig= plot_confusion_matrix(matrix1 , classes=['0','1','2'])

b, t = plt.ylim() # discover the values for bottom and top
b += 0.5 # Add 0.5 to the bottom
t -= 0.5 # Subtract 0.5 from the top
plt.ylim(b, t) # update the ylim(bottom, top) values
plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# 

# In[ ]:





# In[189]:



'''
======== SVC? 
''' 
clf2 = SVC(kernel='linear')

clf2.fit(X_train,y_train)
y_pred2 = clf2.predict(X_test)


# In[200]:


clf2.fit(X_train,y_train.values.ravel())
y_pred2 = clf2.predict(X_test)


# In[201]:


np.unique(target)


# In[199]:


matrix2= confusion_matrix(y_test,y_pred2)

fig= plot_confusion_matrix(matrix2 , classes=['0','1','2'])

b, t = plt.ylim() # discover the values for bottom and top
b += 0.5 # Add 0.5 to the bottom
t -= 0.5 # Subtract 0.5 from the top
plt.ylim(b, t) # update the ylim(bottom, top) values
plt.show()


# In[202]:


print(metrics.classification_report(y_test,y_pred2))


# ============================
# Algoritmos:
# Eu recomendo fortemente que vocês procurem a documentação de cada algoritmo e deem uma lida nos parâmetros pedidos de cada um!
# 
# ==============================
# k-Nearest Neighbors:
# 
# clf = KNeighborsClassifier()
# 
# 
# Support Vector Machine:
# 
# clf = SVC()
# 
# 
# 
# Decision Tree:
# 
# clf = DecisionTreeClassifier()
# 
# 
# 
# Random Forest:
# 
# clf = RandomForestClassifier()
# 
# 
# 
# Treinamento do modelo:
# clf.fit(X_train, y_train.values.ravel())
# 
# 
# 
# Teste:
# y_pred = clf.predict(X_test)
# 
# 
# 
# Matriz de confusão:
# matrix = confusion_matrix(y_test, y_pred)
# 
# 
# fig = plot_confusion_matrix(matrix, classes=['0','1', '2'])
# 
# 
# Métricas de performance:
# metrics.classification_report(y_test, y_pred)
